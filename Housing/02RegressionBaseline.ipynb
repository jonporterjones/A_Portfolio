{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Contains a naive baseline and a learned baseline.  The learned baseline will contain a breif section on explainability - what features have the strongest predictive capacity.  \n",
    "\n",
    "A naive baseline uses no input features and a baseline to this affect is required in any supervised ML model to ensure the model is capable of learning.\n",
    "\n",
    "A learned baseline uses the input features to build a model.  It must beat the naive baseline to demonstrate any capacity to learn.  I have seen instances in my career where machine learned models provided predictions, but the models were unable to learn from the input features.  \n",
    "\n",
    "I like to consider supervised ML a model for continuous improvement.  A baseline model capable of learning from input features could potentially be improved with different techinques (feature engineering, different learning algorithms).  Once a baseline gets eclipsed by an improved model it becomes the new baseline.  As real-world problems get more complex, the possibilities of applying different techniques grow almost infinitely.  Adopting this approach to these types of problems can guide you to know when you have reached the point of diminishing returns and known techniques to improve model performance have been essentially exhausted.  \n",
    "\n",
    "This dataset is a relatively simple and contrived example, but is meant to show the process of a naive baseline, a simple learned baseline, and then additional, and possibly more complex, models meant to be the learned baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features are a np.array of shape (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features are a np.array of shape {housing.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable is a np.array of shape (20640,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target variable is a np.array of shape {housing.target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a random state for reproducible results\n",
    "x_train, x_test, y_train, y_test = train_test_split(housing.data,\n",
    "                                                    housing.target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "(18576, 8)\n",
      "(18576,)\n",
      "Test:\n",
      "(2064, 8)\n",
      "(2064,)\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Test:')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive baseline\n",
    "\n",
    "When solving a problem using Machine Learning, I like to approach it as a problem of continual improvement or experimentation.  The first approach is to compute a naive baseline using only the target value to predict (no consideration of input features).  This can be used to prove that any learned model can actually learn from the input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_baseline_array = np.full(y_test.shape[0], np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_baseline_r2 = r2_score(y_test, naive_baseline_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive baseline without considering input features: -0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Naive baseline without considering input features: {naive_baseline_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive baseline uses the mean of the target value from the training set.  This is the \"predicted\" value used for scoring, all test records will use this as the predicted value.  \n",
    "\n",
    "As expected, the value is 0.  This is consistent with how the r^2 metric works:\n",
    "\n",
    "From sklearn docs:  \n",
    "Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). In the general case when the true y is non-constant, a constant model that always predicts the average y disregarding the input features would get a score of 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned baseline\n",
    "\n",
    "Use a simple linear regression model, the purpose is not to build the best possible model but rather a simple baseline to compare later models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "scores = cross_val_score(linear_model.LinearRegression(), x_train, y_train, cv=5)  # cv is the number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores (training data): [0.60399736 0.59329824 0.59604499 0.5981567  0.60854189]\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Validation Scores (training data): {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of all CV scores (training data): 0.6000078375660196\n"
     ]
    }
   ],
   "source": [
    "print(f'Average of all CV scores (training data): {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit the model on all train data\n",
    "model = linear_model.LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and score the test set to report our baseline performance.\n",
    "y_pred = model.predict(x_test)\n",
    "test_r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline regression using linear regression on test data : 0.628\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline regression using linear regression on test data : {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* Stastical models such as those in sklearn will often expect, and benefit from standardized or scaled data.  Standardization forces all features to have mean 0 and std dev 1 (standard normal); while scaling places data points within boundaries without changing the proportions or distributions of the data.  One would want to explore these options to determine the best approach for pre-processing the data set.\n",
    "\n",
    "* The latitude/longitude parameters are included in this fit, will want to experiment with removing this as using non-linear features in a linear model is not necessarily advised.\n",
    "\n",
    "* We will likely want to experiment with other models and algorithms to beat the baseline built in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic evaluation\n",
    "\n",
    "I would like to evaluate this model to some extent.  Start by looking at the feature coefficients to determine which have the most and least predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.33790154e-01,  9.25129024e-03, -1.03444726e-01,  6.23072795e-01,\n",
       "       -5.76182061e-06, -5.16710201e-03, -4.22392973e-01, -4.35627014e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline is a list of tuples\n",
    "# get the coefficients\n",
    "coefficients = model.coef_\n",
    "# and the intercept\n",
    "intercept = model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_coefficients = dict(zip(housing.feature_names, coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AveBedrms': 0.6230727951255777,\n",
       " 'Longitude': -0.43562701416538097,\n",
       " 'MedInc': 0.433790154007385,\n",
       " 'Latitude': -0.4223929731129496,\n",
       " 'AveRooms': -0.10344472613882673,\n",
       " 'HouseAge': 0.009251290237474749,\n",
       " 'AveOccup': -0.0051671020124161105,\n",
       " 'Population': -5.7618206134466165e-06}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the feature importance by the absolute value of their coefficients.\n",
    "{k: v for k, v in sorted(features_and_coefficients.items(), key=lambda item: abs(item[1]), reverse=True)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "\n",
    "The model coefficients show which features have the strongest effect (positive or negative) on the target variable.\n",
    "\n",
    "AveBedrms is interesting - it doesn't quite follow the plot found in the exploration notebook.  This coefficient shows the affect of AveBedrms while holding all other variables constant.  It does not necessarily imply causation. \n",
    "\n",
    "Longitude and Latitude also are reporting a strong effect on the target variable.  This may or may not be true; Latitude and Longitude might have a linear effect on the target variable or it might not.  This is not to say that Latitude and Longitude would always have a positive effect, but in this case it may coincidentally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
