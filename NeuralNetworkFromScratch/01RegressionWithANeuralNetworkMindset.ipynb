{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with a neural network mindset.\n",
    "\n",
    "This notebook will demonstrate how a linear regression problem can be thought of as an extremely simple neural network.  It is essentially a perceptron with no hidden layers or activation function.\n",
    "\n",
    "This exercise is a pre-cursor to building a complete neaural network from scratch.  This network would contain a hidden layers, activation functions, and multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "In mathematical terms, the regression problem is expressed as:\n",
    "\n",
    "$$z = (\\sum_{n=1}^{n} w^{(n)} \\cdot x^{(n)}) + b$$\n",
    "\n",
    "*z* is the weighted sum of the input features, plus the bias term    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$ is the number of input features  \n",
    "$w^{(n)}$ w are the weights associated with each input feature $x_{n}$  \n",
    "$x^{(n)}$ are the input features  \n",
    "$b$ is the bias term  \n",
    "\n",
    "The parameters to be learned are a $w$ for each input feature and $b$  \n",
    "This will be initialized and then leared through a gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use the same housing dataset as in the Housing folder.  \n",
    "Slightly simplify the features in order to simplify the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Data section\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "# Use the same random state for reproducible results\n",
    "x_train, x_test, y_train, y_test = train_test_split(housing.data,\n",
    "                                                    housing.target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=66)\n",
    "\n",
    "# Remove the latitude and longitude features\n",
    "x_train = x_train[:,0:6]\n",
    "x_test = x_test[:,0:6]\n",
    "\n",
    "# Reshape y to match expected shape\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize x values to assist in convergence\n",
    "# Compute mean and standard deviation\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "\n",
    "# Perform standardization\n",
    "# When standardizing the test set it is important to use the training set mean/std  \n",
    "#  otherwise information about your test data will bleed into your evaluation.\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "A neural network will learn a weight for each input feature plus a single bias term.  \n",
    "This approximation of a neural network will learn the same thing.  \n",
    "\n",
    "First we must initialize the parameters to be learned.  \n",
    "Initialize from a standard normal distribution  \n",
    "then multiply the values by a small constant to ensure small initial weights and prevent exploding (very large) gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(6, 1) * 0.01 # shape is (n_features,1)\n",
    "b = np.random.randn() * 0.01 # bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagation\n",
    "\n",
    "forward propagation - forward propagation in this case is simply computing a predicted value for each input X.  In a larger neural network, this computation would typically be followed by an activation function.  At least in the case of a node in a hidden layer or if the final unit of a neural network is a sigmoid or softmax function.\n",
    "\n",
    "backward propagation - The gradients indicate the direction and magnitude in which the parameter should be adjusted to minimize the loss.  How much will the loss change wrt a small change in the parameter.    \n",
    "The two learned parameters are:  \n",
    "dw - gradient of the loss wrt the weights  \n",
    "db - gradient of the loss wrt the bias   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation.\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (n_samples, n_features)\n",
    "    Y -- true \"label\" vector shape: (n_samples, 1)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0] # Number of samples\n",
    "\n",
    "    # FORWARD PROPAGATION\n",
    "    # compute z.  This will output a predicted value for each sample.\n",
    "    # z will have shape(n_samples, 1)\n",
    "    z = np.dot(X, w) + b\n",
    "\n",
    "    # compute the cost using MSE (Mean Squared Error)\n",
    "    cost = 1/m * (np.sum(np.square(Y-z))) # y-z or y-z won't matter because it is being squared.\n",
    "\n",
    "    # BACKWARD PROPAGATION (compute gradients wrt weights and biases)\n",
    "    dZ = z - Y\n",
    "    dw = 1/m * np.dot(X.T, dZ)\n",
    "    db = 1/m * np.sum(dZ)\n",
    "\n",
    "    gradients = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "        \n",
    "    return gradients, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Now that we have initialized parameter values,  \n",
    "computed the cost function and its gradient,\n",
    "now update the parameters using gradient descent.\n",
    "\n",
    "Learn $w$ and $b$ by minimizing the cost function J.  \n",
    "For a parameter $\\theta$ the update rule is  \n",
    "$ \\theta = \\theta - \\alpha \\text{ } d\\theta$  \n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (n_samples, n_features)\n",
    "    Y -- true \"label\" vector shape: (n_samples, 1))\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (from markdown above)\n",
    "        w = w-(dw*learning_rate)\n",
    "        b = b-(db*learning_rate)\n",
    "\n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Once we have a learned array of weights and a bias term the prediction for a given set of inputs relatively simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict the values for a given set of data using learned weights and bias\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    prediction -- a numpy array containing predicted values (n_samples, 1)\n",
    "    '''\n",
    "    \n",
    "    # In this neural network prototype, there is no activation function so the predictions are simply\n",
    "    #  the dot product of the feature matrix and the weight matrix + the bias term\n",
    "    prediction = np.dot(X, w) + b\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This code will mimic a neural network model.  \n",
    "\n",
    "This \"model\" won't save an artifact.  \n",
    "\n",
    "It will use the optimize function to learn the optimal values for w and b.\n",
    "It will then make predictions on both the train and test sets.\n",
    "it will output the r2 scores on the train and test sets.\n",
    "\n",
    "We can evaluate the r2 score against the test set against the regression baseline.  It should be comporable and it will prove that this extremely simple neural network type of model can learn from the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.552101\n",
      "Cost after iteration 100: 1.320006\n",
      "Cost after iteration 200: 0.747717\n",
      "Cost after iteration 300: 0.665072\n",
      "Cost after iteration 400: 0.649508\n",
      "Cost after iteration 500: 0.643738\n",
      "Cost after iteration 600: 0.639803\n",
      "Cost after iteration 700: 0.636521\n",
      "Cost after iteration 800: 0.633668\n",
      "Cost after iteration 900: 0.631169\n",
      "train score: 0.5246059826612859\n",
      "test score: 0.5423449598458077\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "print_cost = True\n",
    "\n",
    "parameters, _ = optimize(w, b, x_train, y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "# Retrieve parameters w and b from dictionary \"parameters\"\n",
    "w = parameters[\"w\"]\n",
    "b = parameters[\"b\"]\n",
    "\n",
    "# Predict test/train set examples\n",
    "y_train_predictions = predict(w, b, x_train)\n",
    "Y_test_predictions = predict(w, b, x_test)\n",
    "\n",
    "# Print train/test Errors\n",
    "r2_train = r2_score(y_train, y_train_predictions)\n",
    "r2_test = r2_score(y_test, Y_test_predictions)\n",
    "\n",
    "print(f\"train score: {r2_train}\")\n",
    "print(f\"test score: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition/Explainability\n",
    "\n",
    "Output the weights for each feature and the bias term.  \n",
    "The math for predicting an unseen value from the features is relatively simple.\n",
    "Refer to the regresssion calculation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MedInc': array([0.87478891]),\n",
       " 'HouseAge': array([0.22014495]),\n",
       " 'AveRooms': array([-0.1811009]),\n",
       " 'AveBedrms': array([0.15264353]),\n",
       " 'Population': array([0.03281082]),\n",
       " 'AveOccup': array([-0.04342863])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_and_weights = dict(zip(housing.feature_names[0:6], w))\n",
    "features_and_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: 2.0644495015459063\n"
     ]
    }
   ],
   "source": [
    "print(f'bias: {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* This \"model\" was trained for 1000 iterations over the test data.  The cost is decreasing at each iteration and is logged every 100 iterations.\n",
    "* The learned weights and bias were used to predict values for both the train and test datasets.  The predicted test values have an r2 score on par with the baseline, so the model is learning from the input features.\n",
    "* This simplistic example had no need to use a batch size for the training data.  The matrix multiplications were so small that each iteration used the entire training set.  As neural networks and the number of inputs grow larger and more complex these batches over each training iteration become important as way to fit the computations into the total available memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ben-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
